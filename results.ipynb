{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results for Place Name Recognition using Wapiti and NeuroNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows the results for various configurations of Wapiti and NeuroNER tools on several different training and test sets. For now the list of datasets:\n",
    "    -  ACE corpus\n",
    "    -  Conll corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular(title,colnames,rows):\n",
    "    head=title+\"\\n\\t\"\n",
    "    for col in colnames:\n",
    "        head+=col+\"\\t\"\n",
    "    head+=\"\\n\"\n",
    "    for row in rows.keys():\n",
    "        list1=rows[row]\n",
    "        head+=(row+\"\\t\")\n",
    "        for l in list1:\n",
    "            head+=str(l)+\"\\t\"\n",
    "        head+=\"\\n\"\n",
    "    print(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conll dataset\n",
      "\tTrain\tValid\tTest\t\n",
      "LOC\t8297\t2094\t1925\t\n",
      "PER\t11129\t3149\t2773\t\n",
      "MISC\t4593\t1268\t918\t\n",
      "ORG\t10025\t2092\t2496\t\n",
      "\n",
      "ACE dataset\n",
      "\tTrain\tValid\tTest\t\n",
      "LOC\t4176\t942\t684\t\n",
      "ORG\t2470\t551\t293\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conlltrain=open(\"conll2003/train.txt\").read()\n",
    "conllvalid=open(\"conll2003/valid.txt\").read()\n",
    "conlltest=open(\"conll2003/test.txt\").read()\n",
    "ACEtrain=open(\"ACEsplitted/train.txt\").read()\n",
    "ACEvalid=open(\"ACEsplitted/valid.txt\").read()\n",
    "ACEtest=open(\"ACEsplitted/test.txt\").read()\n",
    "setnames=[\"Train\",\"Valid\",\"Test\"]\n",
    "tags=[\"-LOC\",\"-PER\",\"-MISC\",\"-ORG\"]\n",
    "acetags=[\"-LOC\",\"-ORG\"]\n",
    "conllcounts={}\n",
    "for tag in tags:\n",
    "    counts=[]\n",
    "    counts.append(conlltrain.count(tag))\n",
    "    counts.append(conllvalid.count(tag))\n",
    "    counts.append(conlltest.count(tag))\n",
    "    conllcounts[tag[1:]]=counts\n",
    "title=\"Conll dataset\"\n",
    "tabular(title,setnames,conllcounts)\n",
    "acecounts={}\n",
    "for tag in acetags:\n",
    "    counts=[]\n",
    "    counts.append(ACEtrain.count(tag))\n",
    "    counts.append(ACEvalid.count(tag))\n",
    "    counts.append(ACEtest.count(tag))\n",
    "    acecounts[tag[1:]]=counts  \n",
    "title=\"ACE dataset\"\n",
    "tabular(title,setnames,acecounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculates and prints precision and recall for the resf file only taking into consideration the tags in  tagf var.\n",
    "def acc(resf,tagf):\n",
    "    t=0.0\n",
    "    c=0\n",
    "    tot=0\n",
    "    ptot=0\n",
    "    p=0.0\n",
    "    res1=open(resf).readlines()\n",
    "    for line in res1:\n",
    "        line1=line.split()\n",
    "        if len(line1)>2:\n",
    "            if line1[-2] in tagf:\n",
    "                if line1[-2]==line1[-1]:\n",
    "                    t+=1\n",
    "                c+=1\n",
    "            if line1[-1] in tagf:\n",
    "                if line1[-1]==line1[-2]:\n",
    "                    p+=1\n",
    "                ptot+=1\n",
    "            tot+=1\n",
    "    print(\"Total predictions: \"+str(ptot))\n",
    "    print(\"Total entities: \"+ str(c))\n",
    "    rec=t/c\n",
    "    pre=p/ptot\n",
    "    print(\"recall: \"+str(rec))\n",
    "    print(\"precision: \"+str(pre))\n",
    "    return pre,rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculates and prints precision and recall for the resf file only taking into consideration the tags in  tagf var.\n",
    "def acc2(resf):\n",
    "    t=0.0\n",
    "    c=0\n",
    "    tot=0\n",
    "    ptot=0\n",
    "    p=0.0\n",
    "    res1=open(resf).readlines()\n",
    "    for line in res1:\n",
    "        line1=line.split()\n",
    "        if len(line1)>2:\n",
    "            if line1[-2]!=\"O\" and line1[-2]!=\"0\":\n",
    "                if line1[-2]==line1[-1]:\n",
    "                    t+=1\n",
    "                c+=1\n",
    "            if line1[-1]!=\"O\" and line1[-1]!=\"0\":\n",
    "                if line1[-1]==line1[-2]:\n",
    "                    p+=1\n",
    "                ptot+=1\n",
    "            tot+=1\n",
    "    print(\"Total predictions: \"+str(ptot))\n",
    "    print(\"Total entities: \"+ str(c))\n",
    "    rec=t/c\n",
    "    pre=p/ptot\n",
    "    print(\"recall: \"+str(rec))\n",
    "    print(\"precision: \"+str(pre))\n",
    "    return pre,rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculates and returns fbeta score using the parameters\n",
    "def fbeta(beta,pre,rec):\n",
    "    den = beta*beta* pre + rec\n",
    "    nom = (beta*beta+1)*pre*rec\n",
    "    return nom/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagsace=[\"GPE\",\"LOC\"]\n",
    "tagscon=[\"I-LOC\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE:*** User must change the value of the resultsfile variable for each result accordingly. I used the default addresses for them. The program will not work if the address is not given properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)Wapiti Results\n",
    "\n",
    "Wapiti is a ML tool used generally for sequence labelling, such as POS tagging, NER etc. Wapiti uses Conditional Random Fields which are proven to be very powerful for labeling sequential data. Below is the link to have more information about the tool:\n",
    "\n",
    "https://wapiti.limsi.fr/manual.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for Conll alone\n",
    "\n",
    "Results below use the Conll dataset and the predefined Conll features for learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 1\n",
    "\n",
    "* **Training set**: Conll training set \n",
    "\n",
    "* **Test set**: Conll testa,testb\n",
    "* **Pattern file**: nppattern.txt\n",
    "\n",
    "* **Configurations**: L1 norm penalty 5 \n",
    "\n",
    "* **Terminal call for training**: `wapiti train -p patternfile -1 5 trainfile modelfile`\n",
    "* **Terminal call for prediction**: `wapiti label -m modelfile testfile outputfile`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**testa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 2158\n",
      "Total entities: 2094\n",
      "recall: 0.8720152817574021\n",
      "precision: 0.8461538461538461\n",
      "F1 score:0.8588899341486358\n"
     ]
    }
   ],
   "source": [
    "resultsfile=\"results/resa.txt\"\n",
    "beta=1\n",
    "pre,rec=acc(resultsfile,tagscon)\n",
    "f=fbeta(beta,pre,rec)\n",
    "print(\"F1 score:\"+str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**testb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 1946\n",
      "Total entities: 1919\n",
      "recall: 0.8134445023449713\n",
      "precision: 0.802158273381295\n",
      "F1 score:0.8077619663648123\n"
     ]
    }
   ],
   "source": [
    "resultsfile=\"results/resb.txt\"\n",
    "beta=1\n",
    "pre,rec=acc(resultsfile,tagscon)\n",
    "f=fbeta(beta,pre,rec)\n",
    "print(\"F1 score:\"+str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 2\n",
    "\n",
    "* **Training set**: Conll training set \n",
    "\n",
    "* **Test set**: Conll testa,testb\n",
    "* **Pattern file**: nppattern.txt\n",
    "\n",
    "* **Configurations**: default mode with no L1 penalty \n",
    "\n",
    "* **Terminal call for training**: `wapiti train -p patternfile trainfile modelfile`\n",
    "* **Terminal call for prediction**: `wapiti label -m modelfile testfile outputfile`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**testa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 2098\n",
      "Total entities: 2094\n",
      "recall: 0.9164278892072588\n",
      "precision: 0.9146806482364156\n",
      "F1 score:0.9155534351145037\n"
     ]
    }
   ],
   "source": [
    "resultsfile=\"results/resa2.txt\"\n",
    "beta=1\n",
    "pre,rec=acc(resultsfile,tagscon)\n",
    "f=fbeta(beta,pre,rec)\n",
    "print(\"F1 score:\"+str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**testb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 1878\n",
      "Total entities: 1919\n",
      "recall: 0.853048462741011\n",
      "precision: 0.8716719914802982\n",
      "F1 score:0.8622596786937057\n"
     ]
    }
   ],
   "source": [
    "resultsfile=\"results/resb2.txt\"\n",
    "beta=1\n",
    "pre,rec=acc(resultsfile,tagscon)\n",
    "f=fbeta(beta,pre,rec)\n",
    "print(\"F1 score:\"+str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 3\n",
    "\n",
    "* **Training set**: Conll training set (sentence splitted version)\n",
    "\n",
    "* **Test set**: Conll testa,testb\n",
    "* **Pattern file**: nppattern.txt\n",
    "\n",
    "* **Configurations**: default mode with no L1 penalty\n",
    "\n",
    "* **Terminal call for training**: `wapiti train -p patternfile trainfile modelfile`\n",
    "* **Terminal call for prediction**: `wapiti label -m modelfile testfile outputfile`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** testa **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 2108\n",
      "Total entities: 2094\n",
      "recall: 0.9202483285577842\n",
      "precision: 0.9141366223908919\n",
      "F1 score:0.917182294145645\n"
     ]
    }
   ],
   "source": [
    "resultsfile=\"results/wapresa\"\n",
    "beta=1\n",
    "pre,rec=acc(resultsfile,tagscon)\n",
    "f=fbeta(beta,pre,rec)\n",
    "print(\"F1 score:\"+str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**testb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 1896\n",
      "Total entities: 1919\n",
      "recall: 0.8520062532569046\n",
      "precision: 0.8623417721518988\n",
      "F1 score:0.8571428571428572\n"
     ]
    }
   ],
   "source": [
    "resultsfile=\"results/wapresb\"\n",
    "beta=1\n",
    "pre,rec=acc(resultsfile,tagscon)\n",
    "f=fbeta(beta,pre,rec)\n",
    "print(\"F1 score:\"+str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for ACE alone\n",
    "\n",
    "There are many entity types available in the ACE dataset. For the purpose of our project we only consider the entities with tags ORG, GPE and LOC. As in the case for Conll we ignore the boundaries (BIO representation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 4\n",
    "\n",
    "No features are given to the CRF learner. These can be considered as the baseline performance of Wapiti on ACE alone.\n",
    "A lot of improvements can be done over these scores.\n",
    "* **Training set**: 90% of ACE corpus (no features except for surface form + regex)\n",
    "* **Test set**:  10% of ACE corpus\n",
    "* **Pattern file**: acepats\n",
    "* **Configurations**: L1 penalty 1\n",
    "\n",
    "* **Terminal call for training**: `wapiti train -p -1 1 patternfile trainfile modelfile`\n",
    "* **Terminal call for prediction**: `wapiti label -m modelfile testfile outputfile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 358\n",
      "Total entities: 284\n",
      "recall: 0.8380281690140845\n",
      "precision: 0.664804469273743\n",
      "F1 score:0.7414330218068536\n"
     ]
    }
   ],
   "source": [
    "resultsfile=\"results/res1\"\n",
    "beta=1\n",
    "pre,rec=acc(resultsfile,tagsace)\n",
    "f=fbeta(beta,pre,rec)\n",
    "print(\"F1 score:\"+str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 5\n",
    "\n",
    "* **Training set**: 90% of ACE corpus (no features except for surface form + regex)\n",
    "* **Test set**:  10% of ACE corpus\n",
    "* **Pattern file**: acepats\n",
    "* **Configurations**: Default mode with no penalty\n",
    "\n",
    "* **Terminal call for training**: `wapiti train -p patternfile trainfile modelfile`\n",
    "* **Terminal call for prediction**: `wapiti label -m modelfile testfile outputfile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 349\n",
      "Total entities: 284\n",
      "recall: 0.8450704225352113\n",
      "precision: 0.6876790830945558\n",
      "F1 score:0.7582938388625592\n"
     ]
    }
   ],
   "source": [
    "resultsfile=\"results/res2\"\n",
    "beta=1\n",
    "pre,rec=acc(resultsfile,tagsace)\n",
    "f=fbeta(beta,pre,rec)\n",
    "print(\"F1 score:\"+str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 6\n",
    "\n",
    "* **Training set**: 100% of Conll and 70% of ACE corpus (no features except for surface form + regex)\n",
    "* **Test set**:  20% of ACE corpus\n",
    "* **Pattern file**: acepats\n",
    "* **Configurations**: Default mode with no penalty\n",
    "\n",
    "* **Terminal call for training**: `wapiti train -p patternfile trainfile modelfile`\n",
    "* **Terminal call for prediction**: `wapiti label -m modelfile testfile outputfile`\n",
    "\n",
    "results are in the \"results/merres*.txt\" files (5 files for models that are slightly different). Results are similar for all 5 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 1679\n",
      "Total entities: 1897\n",
      "recall: 0.7248286768581972\n",
      "precision: 0.8189398451459202\n",
      "F1 score:0.7690156599552573\n"
     ]
    }
   ],
   "source": [
    "resultsfile=\"results/merres.txt\"\n",
    "beta=1\n",
    "pre,rec=acc2(resultsfile)\n",
    "f=fbeta(beta,pre,rec)\n",
    "print(\"F1 score:\"+str(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 1727\n",
      "Total entities: 1897\n",
      "recall: 0.7311544544016869\n",
      "precision: 0.8031268094962363\n",
      "F1 score:0.7654525386313467\n"
     ]
    }
   ],
   "source": [
    "resultsfile=\"results/merres2.txt\"\n",
    "beta=1\n",
    "pre,rec=acc2(resultsfile)\n",
    "f=fbeta(beta,pre,rec)\n",
    "print(\"F1 score:\"+str(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 1762\n",
      "Total entities: 1897\n",
      "recall: 0.7337901950448076\n",
      "precision: 0.7900113507377979\n",
      "F1 score:0.7608636239409674\n"
     ]
    }
   ],
   "source": [
    "resultsfile=\"results/merres3.txt\"\n",
    "beta=1\n",
    "pre,rec=acc2(resultsfile)\n",
    "f=fbeta(beta,pre,rec)\n",
    "print(\"F1 score:\"+str(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 1711\n",
      "Total entities: 1897\n",
      "recall: 0.7285187137585661\n",
      "precision: 0.8077147866744594\n",
      "F1 score:0.7660753880266077\n"
     ]
    }
   ],
   "source": [
    "resultsfile=\"results/merres4.txt\"\n",
    "beta=1\n",
    "pre,rec=acc2(resultsfile)\n",
    "f=fbeta(beta,pre,rec)\n",
    "print(\"F1 score:\"+str(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using memm rather than crf\n",
      "\n",
      "Total predictions: 1681\n",
      "Total entities: 1897\n",
      "recall: 0.7137585661570901\n",
      "precision: 0.8054729327781083\n",
      "F1 score:0.7568474007825601\n"
     ]
    }
   ],
   "source": [
    "print(\"Using memm rather than crf\\n\")\n",
    "resultsfile=\"results/merres5.txt\"\n",
    "beta=1\n",
    "pre,rec=acc2(resultsfile)\n",
    "f=fbeta(beta,pre,rec)\n",
    "print(\"F1 score:\"+str(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 765\n",
      "Total entities: 942\n",
      "recall: 0.7346072186836518\n",
      "precision: 0.9045751633986928\n",
      "F1 score:0.810779144698301\n"
     ]
    }
   ],
   "source": [
    "## results for LOC tag alone\n",
    "resultsfile=\"results/reslocalone\"\n",
    "beta=1\n",
    "pre,rec=acc(resultsfile,[\"B-LOC\",\"I-LOC\"])\n",
    "f=fbeta(beta,pre,rec)\n",
    "print(\"F1 score:\"+str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a larger window for capitalization feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 580\n",
      "Total entities: 684\n",
      "recall: 0.75\n",
      "precision: 0.8844827586206897\n",
      "F1 score:0.8117088607594938\n"
     ]
    }
   ],
   "source": [
    "## results for LOC tag alone\n",
    "resultsfile=\"results/largewindowres\"\n",
    "beta=1\n",
    "pre,rec=acc(resultsfile,[\"B-LOC\",\"I-LOC\"])\n",
    "f=fbeta(beta,pre,rec)\n",
    "print(\"F1 score:\"+str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) NeuroNER Results\n",
    "\n",
    "NeuroNER is a Named Entity Recognition Tool that uses Bi-LSTM CRF with word and character embeddings.\n",
    "\n",
    "The tool gives state-of-art results for the Conll dataset\n",
    "\n",
    "More information is available at:\n",
    "\n",
    "http://neuroner.com/\n",
    "\n",
    "Also the github repo https://github.com/Franck-Dernoncourt/NeuroNER has very detailed information about the tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Model on Conll testset\n",
    "\n",
    "As the pretrained model is trained on Conll training set, the model performs very well on Conll test set.\n",
    "\n",
    "Confusion matrix for this result is in the \"results/NeuroNer_Conlltest_Confmat.pdf\" file in our github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 46435 tokens with 5648 phrases; found: 5663 phrases; correct: 5127.\n",
      "accuracy:  97.89%; precision:  90.54%; recall:  90.78%; FB1:  90.66\n",
      "              LOC: precision:  92.53%; recall:  92.81%; FB1:  92.67  1673\n",
      "             MISC: precision:  81.12%; recall:  80.20%; FB1:  80.66  694\n",
      "              ORG: precision:  86.98%; recall:  89.28%; FB1:  88.12  1705\n",
      "              PER: precision:  96.35%; recall:  94.81%; FB1:  95.57  1591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res1=open(\"results/NeuroNer_Conlltestres.txt\").read()\n",
    "print(res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Model on ACE testset\n",
    "\n",
    "As the model is trained on a different dataset the scores drop significantly, which suggests that pretrained models perform poorly on different domains with little entity overlaps. Another approach is that pretrained model on Conll has the PER and MISC tags which are not available in our ACE corpus. Thus the results are misleading and this approach is not meaningful. So we change the labels in the Conll dataset and train the model again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model on Conll and testing on ACE test set\n",
    "\n",
    "We start testing our model on ACE using Conll dataset for the training. We merged the conll train test and validation sets into a single corpus. Training is done on this corpus, for validation and testing we use 10% and 20% of ACE respectively. Confusion matrix address: \n",
    "\n",
    "\"**results/NeuroNer_Conlltrain_ACEtest_confmat.pdf**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 49689 tokens with 1408 phrases; found: 1273 phrases; correct: 845.\n",
      "accuracy:  97.47%; precision:  66.38%; recall:  60.01%; FB1:  63.04\n",
      "              LOC: precision:  71.54%; recall:  63.31%; FB1:  67.17  931\n",
      "              ORG: precision:  52.34%; recall:  50.28%; FB1:  51.29  342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contracetesres1=open(\"results/NeuroNer_Conlltrain_ACEtest_res.txt\").read()\n",
    "print(contracetesres1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model on using ACE dataset alone\n",
    "\n",
    "Next we trained the NeuroNer tool by splitting the ACE corpus into 3 sets using corpsplitter.py:\n",
    "\n",
    "* train.txt\n",
    "* valid.txt\n",
    "* test.txt\n",
    "\n",
    "Below are the results of the trained model on ACE test set. As the size of test and valid sets are small the results can be misleading. They are only shown to give a rough idea about the success of the model. Confusion matrix is included in the github with name \"**results/ACE_test_confmat.pdf**\". \n",
    "\n",
    "Training the model using the ACE dataset increases the scores significantly. Results suggests NER systems perform well when the training and test data is from the same domain or same source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 49689 tokens with 1408 phrases; found: 1521 phrases; correct: 1221.\n",
      "accuracy:  98.69%; precision:  80.28%; recall:  86.72%; FB1:  83.37\n",
      "              LOC: precision:  83.20%; recall:  91.35%; FB1:  87.09  1155\n",
      "              ORG: precision:  71.04%; recall:  73.03%; FB1:  72.02  366\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aceres1=open(\"results/ACEtrain_testres.txt\").read()\n",
    "print(aceres1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model on  ACE and Conll datasets combined\n",
    "\n",
    "In this step we merged the 2 datasets( 100% of Conll and 70 % of ACE) and trained the model using this larger corpus.\n",
    "Testing is done on 20 % of ACE and validation uses the 10%.\n",
    "\n",
    "As the corpus size is doubled training times increased accordingly. Each epoch takes around 900 seconds. Below are the results for the ANN trained for *34* epochs calculated using token metric (not Conll metric). The results show that we obtain an increase in the F1-score by merging two corpora. \n",
    "\n",
    "* **\"results/mergedtrain_ACEtest_confmat.pdf\"** is the confusion matrix file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 49689 tokens with 1408 phrases; found: 1443 phrases; correct: 1214.\n",
      "accuracy:  98.85%; precision:  84.13%; recall:  86.22%; FB1:  85.16\n",
      "              LOC: precision:  85.69%; recall:  92.21%; FB1:  88.83  1132\n",
      "              ORG: precision:  78.46%; recall:  68.54%; FB1:  73.16  311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resmerged=open(\"results/mergedtrain_ACEtestres.txt\").read()\n",
    "print(resmerged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally to be more confident with the results we have obtained for this model we changed the test set without training the model again. Below are the results for that experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 48051 tokens with 1276 phrases; found: 1216 phrases; correct: 977.\n",
      "accuracy:  98.84%; precision:  80.35%; recall:  76.57%; FB1:  78.41\n",
      "              LOC: precision:  84.67%; recall:  89.05%; FB1:  86.80  874\n",
      "              ORG: precision:  69.30%; recall:  53.26%; FB1:  60.23  342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resmerged2=open(\"results/ACEconlltrain_finaltest.txt\").read()\n",
    "print(resmerged2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pos tag ekleyebilirim ACE datasetine\n",
    "sent=\" Orlando is the biggest city in Turkey.\"\n",
    "text=nltk.word_tokenize(\"We are going out. Just you and me.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gazetteers as gz\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import brown as bn\n",
    "from nltk.corpus import names,stopwords, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1211\n",
      "7944\n",
      "4850\n"
     ]
    }
   ],
   "source": [
    "print(len(gz.words()))\n",
    "print(len(names.words()))\n",
    "print(len(stopwords.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180707\n",
      "346121\n"
     ]
    }
   ],
   "source": [
    "val=open('merged corpus/valid.txt').read()\n",
    "test=open('merged corpus/test.txt').read()\n",
    "print(len(val))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
